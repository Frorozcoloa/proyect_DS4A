{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Companion Notebook for Scraping Twitter Using Tweepy\n",
    "\n",
    "Package Github: https://github.com/tweepy/tweepy\n",
    "\n",
    "Package Documentation: https://tweepy.readthedocs.io/en/latest/\n",
    "\n",
    "Article Read-Along: https://towardsdatascience.com/how-to-scrape-more-information-from-tweets-on-twitter-44fd540b8a1f\n",
    "\n",
    "### Notebook Author: Martin Beck\n",
    "#### Information current as of August, 13th 2020\n",
    "<b> Dependencies:</b> Make sure Tweepy is already installed in your Python environment. If not, you can pip install Tweepy to install the package. If you want more information on setting up I have an article [here](https://towardsdatascience.com/how-to-scrape-tweets-from-twitter-59287e20f0f1) that goes into deeper detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook's Table of Contents<a name=\"TOC\"></a>\n",
    "<br>\n",
    "<b>This companion notebook is meant to build on the scraping article and article notebook as it covers more scenarios that may come up and provides more examples.</b>\n",
    "\n",
    "0. [Credentials and Authorization](#Section0)\n",
    "<br>Setting up credentials and authorization in order to utilize Tweepy\n",
    "1. [Getting More Information From Tweets](#Section1)\n",
    "<br>How to scrape more information from tweets such as favorite count, retweet count, if they're replying to someone else, if turned on the coordinates of where the tweet came from, etc.\n",
    "2. [Getting User Information From Tweets](#Section2)\n",
    "<br>How to scrape user information from tweets such as their follower count, total amount of tweets, if they're a verified user, location of where account is registered, etc.\n",
    "3. [Scraping Tweets With Advanced Queries](#Section3)\n",
    "<br>How to scrape for tweets using deeper queries such as searching by language of tweets, tweets within a certain location, tweets within specific date ranges, top tweets, etc.\n",
    "4. [Putting It All Together](#Section4)\n",
    "<br>Showcasing how you can mix and match the methods shown above to create queries that'll fulfill your data needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports for Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pip install Tweepy if you don't already have the package\n",
    "# !pip install tweepy\n",
    "\n",
    "# Imports\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Credentials and Authorization<a name=\"Section0\"></a>\n",
    "[Return to Table of Contents](#TOC)\n",
    "<br>Tweepy requires credentials before you can utilize its API. The below code helps setup the notebook for authorization. I already have an an article covering setting up Tweepy and getting credentials [here](https://towardsdatascience.com/how-to-scrape-tweets-from-twitter-59287e20f0f1) if further instructions are needed.\n",
    "\n",
    "You don't necessarily have to create a credentials file, however if you find youself sharing Tweepy code to other parties I recommend it so you don't accidentally share your credentials. Otherwise skip the below cell and just enter your credentials in and have them hardcoded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>consumer_key</td>\n",
       "      <td>XXXXXXXXXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>consumer_secret</td>\n",
       "      <td>XXXXXXXXXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>access_token</td>\n",
       "      <td>XXXXXXXXXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>access_secret</td>\n",
       "      <td>XXXXXXXXXXX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name          key\n",
       "0     consumer_key  XXXXXXXXXXX\n",
       "1  consumer_secret  XXXXXXXXXXX\n",
       "2     access_token  XXXXXXXXXXX\n",
       "3    access_secret  XXXXXXXXXXX"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading in from csv file\n",
    "\n",
    "credentials_df = pd.read_csv('credentials.csv',header=None,names=['name','key'])\n",
    "\n",
    "credentials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials from csv file\n",
    "\n",
    "consumer_key = credentials_df.loc[credentials_df['name']=='consumer_key','key'].iloc[0]\n",
    "consumer_secret = credentials_df.loc[credentials_df['name']=='consumer_secret','key'].iloc[0]\n",
    "access_token = credentials_df.loc[credentials_df['name']=='access_token','key'].iloc[0]\n",
    "access_token_secret = credentials_df.loc[credentials_df['name']=='access_secret','key'].iloc[0]\n",
    "\n",
    "# Credentials hardcoded\n",
    "\n",
    "# consumer_key = \"XXXXX\"\n",
    "# consumer_secret = \"XXXXX\"\n",
    "# access_token = \"XXXXX\"\n",
    "# access_token_secret = \"XXXXXX\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting More Information From Tweets<a name=\"Section1\"></a>\n",
    "[Return to Table of Contents](#TOC)\n",
    "<br>List of information available in tweet object with Tweepy. This is not an exhaustive list but does contain a majority of the available information. If you want an exhaustive list of everything contained in the tweet object there's documentation [here](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/tweet-object) describing all the attributes. \n",
    "\n",
    "String versions of Id's (e.g., id_str, in_reply_to_status_id_str) are used instead to best keep data integrity as there is a possibility for Id's stored as integers to be cut off.\n",
    "\n",
    "* tweet.user <b>User information is covered in part 2 in greater detail</b><br><br>\n",
    "\n",
    "* tweet.full_text: <b>Text content of tweet when API is told to pull all contents of tweets that have more than 140 characters</b><br><br>\n",
    "\n",
    "* tweet.text: Text content of tweet\n",
    "* tweet.created_at: Date tweet was created\n",
    "* tweet.id_str: Id of tweet\n",
    "* tweet.user.screen_name: Username of tweet's author\n",
    "* tweet.coordinates: Geographic location as reported by user or client. May be null that is why extract_coordinates function below was created\n",
    "* tweet.place: Indicates place associated with tweet where user signed up with like Las Vegas, NV. May be null that so extract_place function below was created\n",
    "* tweet.retweet_count: Count of retweets\n",
    "* tweet.favorite_count: Count of favorites\n",
    "* tweet.lang: Indicates a BCP 47 language identifier corresponding to machine detected language of tweet text.\n",
    "* tweet.source: Source where tweet was posted through. Ex: Twitter Web Client\n",
    "* tweet.in_reply_to_status_id_str: If a tweet is a reply, the original tweet's id. Can be null if tweet is not a reply\n",
    "* tweet.in_reply_to_user_id_str: If a tweet is a reply, string representation of original tweet's user id\n",
    "* tweet.is_quote_status: If tweet is a quote tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query by Username\n",
    "I created three functions to build off of based off of various scenarios that are likely to happen for someone scraping tweets from users. After each function I call them to showcase an example of them being used.\n",
    "\n",
    "#### F0. extract_coordinates and extract_place\n",
    "These functions check for if a tweet has either coordinate information or place information and extract the pertinent information from their json. These are separate functions because they can be nullable so it's important to check first if they have them then to extract and replace in the dataframe.\n",
    "\n",
    "#### F1. scrape_user_tweets\n",
    "This function scrapes a single users tweets and exports the data as a csv or excel file\n",
    "\n",
    "#### F2. scrape_multiple_users_multifile\n",
    "This function scrapes multiple users based on a list and exports separate csv or excel files per user.\n",
    "\n",
    "#### F3. scrape_multiple_users_singlefile\n",
    "This function scrapes multiple users based on a list and exports one csv or excel file containing all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function created to extract coordinates from tweet if it has coordinate info\n",
    "# Tweets tend to have null so important to run check\n",
    "# Make sure to run this cell as it is used in a lot of different functions below\n",
    "def extract_coordinates(row):\n",
    "    if row['Tweet Coordinates']:\n",
    "        return row['Tweet Coordinates']['coordinates']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function created to extract place such as city, state or country from tweet if it has place info\n",
    "# Tweets tend to have null so important to run check\n",
    "# Make sure to run this cell as it is used in a lot of different functions below\n",
    "def extract_place(row):\n",
    "    if row['Place Info']:\n",
    "        return row['Place Info'].full_name\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_user_tweets(username, max_tweets):\n",
    "    # Creation of query method using parameters\n",
    "    tweets = tweepy.Cursor(api.user_timeline,id=username).items(max_tweets)\n",
    "\n",
    "    # List comprehension pulling chosen tweet information from tweets iterable object\n",
    "    # Add or remove tweet information you want in the below list comprehension\n",
    "    tweets_list = [[tweet.text, tweet.created_at, tweet.id_str, tweet.user.screen_name, tweet.coordinates,\n",
    "                   tweet.place, tweet.retweet_count, tweet.favorite_count, tweet.lang,\n",
    "                   tweet.source, tweet.in_reply_to_status_id_str, \n",
    "                    tweet.in_reply_to_user_id_str, tweet.is_quote_status,\n",
    "                    ] for tweet in tweets]\n",
    "\n",
    "    # Creation of dataframe from tweets_list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list,columns=['Tweet Text', 'Tweet Datetime', 'Tweet Id', 'Twitter @ Name', 'Tweet Coordinates', 'Place Info',\n",
    "                                                 'Retweets', 'Favorites', 'Language', 'Source', 'Replied Tweet Id',\n",
    "                                                  'Replied Tweet User Id Str', 'Quote Status Bool'])\n",
    "    \n",
    "    # Checks if there are coordinates attached to tweets, if so extracts them\n",
    "    tweets_df['Tweet Coordinates'] = tweets_df.apply(extract_coordinates,axis=1)\n",
    "    \n",
    "    # Checks if there is place information available, if so extracts them\n",
    "    tweets_df['Place Info'] = tweets_df.apply(extract_place,axis=1)\n",
    "    \n",
    "    # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "    tweets_df.to_csv('{}-tweets.csv'.format(username), sep=',', index = False)\n",
    "#     tweets_df.to_excel('{}-tweets.xlsx'.format(username), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating example username to scrape from\n",
    "username = 'random'\n",
    "\n",
    "# Max recent tweets pulls x amount of most recent tweets from that user\n",
    "max_tweets = 150\n",
    "\n",
    "# Function will scrape username, attempt to pull max_tweet amount, and create csv/excel file from data.\n",
    "scrape_user_tweets(username,max_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_users_multifile(username_list, max_tweets_per):\n",
    "    # Looping through each username in user list\n",
    "    \n",
    "    for username in username_list:   \n",
    "        # Creation of query method using parameters\n",
    "        tweets = tweepy.Cursor(api.user_timeline,id=username).items(max_tweets_per)\n",
    "\n",
    "        # List comprehension pulling chosen tweet information from tweets iterable object\n",
    "        # Add or remove tweet information you want in the below list comprehension\n",
    "        tweets_list = [[tweet.text, tweet.created_at, tweet.id_str, tweet.user.screen_name, tweet.coordinates,\n",
    "                   tweet.place, tweet.retweet_count, tweet.favorite_count, tweet.lang,\n",
    "                   tweet.source, tweet.in_reply_to_status_id_str, \n",
    "                    tweet.in_reply_to_user_id_str, tweet.is_quote_status,] for tweet in tweets]\n",
    "\n",
    "        # Creation of dataframe from tweets_list\n",
    "        # Add or remove columns as you remove tweet information\n",
    "        tweets_df = pd.DataFrame(tweets_list,columns=['Tweet Text', 'Tweet Datetime', 'Tweet Id', 'Twitter @ Name', 'Tweet Coordinates', 'Place Info',\n",
    "                                                 'Retweets', 'Favorites', 'Language', 'Source', 'Replied Tweet Id',\n",
    "                                                  'Replied Tweet User Id Str', 'Quote Status Bool'])\n",
    "        \n",
    "        # Checks if there are coordinates attached to tweets, if so extracts them\n",
    "        tweets_df['Tweet Coordinates'] = tweets_df.apply(extract_coordinates,axis=1)\n",
    "        \n",
    "        # Checks if there is place information available, if so extracts them\n",
    "        tweets_df['Place Info'] = tweets_df.apply(extract_place,axis=1)\n",
    "        \n",
    "        # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "        tweets_df.to_csv('{}-tweets.csv'.format(username), sep=',', index = False)\n",
    "#         tweets_df.to_excel('{}-tweets.xlsx'.format(username), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating example user list with 3 users\n",
    "user_name_list = ['jack','billgates','random']\n",
    "\n",
    "# Max recent tweets pulls x amount of most recent tweets from that user\n",
    "max_tweets_per = 150\n",
    "\n",
    "# Function will scrape each user, attempting to pull max_tweet amount, and create csv/excel file per user.\n",
    "scrape_multiple_users_multifile(user_name_list, max_tweets_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_users_singlefile(username_list, max_tweets_per):\n",
    "    # Creating master list to contain all tweets\n",
    "    master_tweets_list = []\n",
    "    \n",
    "    # Looping through each username in user list\n",
    "    for username in user_name_list:\n",
    "        # Creation of query method using parameters\n",
    "        tweets = tweepy.Cursor(api.user_timeline,id=username).items(max_tweets_per)\n",
    "        \n",
    "        # List comprehension pulling chosen tweet information from tweets iterable object\n",
    "        # Appending new tweets per user into the master tweet list\n",
    "        # Add or remove tweet information you want in the below list comprehension\n",
    "        for tweet in tweets:\n",
    "            master_tweets_list.append((tweet.text, tweet.created_at, tweet.id_str, tweet.user.screen_name, tweet.coordinates,\n",
    "                   tweet.place, tweet.retweet_count, tweet.favorite_count, tweet.lang,\n",
    "                   tweet.source, tweet.in_reply_to_status_id_str, \n",
    "                    tweet.in_reply_to_user_id_str, tweet.is_quote_status))\n",
    "        \n",
    "    # Creation of dataframe from tweets_list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(master_tweets_list,columns=['Tweet Text', 'Tweet Datetime', 'Tweet Id', 'Twitter @ Name', 'Tweet Coordinates', 'Place Info',\n",
    "                                                 'Retweets', 'Favorites', 'Language', 'Source', 'Replied Tweet Id',\n",
    "                                                  'Replied Tweet User Id Str', 'Quote Status Bool'])\n",
    "\n",
    "    # Checks if there are coordinates attached to tweets, if so extracts them\n",
    "    tweets_df['Tweet Coordinates'] = tweets_df.apply(extract_coordinates,axis=1)\n",
    "    \n",
    "    # Checks if there is place information available, if so extracts them\n",
    "    tweets_df['Place Info'] = tweets_df.apply(extract_place,axis=1)\n",
    "    \n",
    "    # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "    tweets_df.to_csv('multi-user-tweets.csv', sep=',', index = False)\n",
    "#     tweets_df.to_excel('multi-user-tweets.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating example user list with 3 users\n",
    "user_name_list = ['jack','billgates','random']\n",
    "\n",
    "# Max recent tweets pulls x amount of most recent tweets from that user\n",
    "max_tweets_per = 150\n",
    "\n",
    "# Function will scrape each user, attempting to pull max_tweet amount, and create one csv/excel file containing all data name multi-user-tweets.\n",
    "scrape_multiple_users_singlefile(user_name_list, max_tweets_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allowing API to Access up to 280 Characters From Tweets\n",
    "\n",
    "In the cursor parameters add tweet_mode='extended' to access tweet text that goes beyond Twitter's original 140 character limit.\n",
    "\n",
    "If tweet_mode is set to extended the tweet attribute tweet.text becomes tweet.full_text isntead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_extended_tweets(username, max_tweets):\n",
    "    # Creation of query method using parameters\n",
    "    tweets = tweepy.Cursor(api.user_timeline,id=username, tweet_mode='extended').items(max_tweets)\n",
    "\n",
    "    # List comprehension pulling chosen tweet information from tweets iterable object\n",
    "    # Add or remove tweet information you want in the below list comprehension\n",
    "    tweets_list = [[tweet.full_text, tweet.created_at, tweet.id_str, tweet.user.screen_name, tweet.coordinates,\n",
    "                   tweet.place, tweet.retweet_count, tweet.favorite_count, tweet.lang,\n",
    "                   tweet.source, tweet.in_reply_to_status_id_str, \n",
    "                    tweet.in_reply_to_user_id_str, tweet.is_quote_status,\n",
    "                    ] for tweet in tweets]\n",
    "\n",
    "    # Creation of dataframe from tweets_list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list,columns=['Tweet Text', 'Tweet Datetime', 'Tweet Id', 'Twitter @ Name', 'Tweet Coordinates', 'Place Info',\n",
    "                                                 'Retweets', 'Favorites', 'Language', 'Source', 'Replied Tweet Id',\n",
    "                                                  'Replied Tweet User Id Str', 'Quote Status Bool'])\n",
    "    \n",
    "    # Checks if there are coordinates attached to tweets, if so extracts them\n",
    "    tweets_df['Tweet Coordinates'] = tweets_df.apply(extract_coordinates,axis=1)\n",
    "    \n",
    "    # Checks if there is place information available, if so extracts them\n",
    "    tweets_df['Place Info'] = tweets_df.apply(extract_place,axis=1)\n",
    "    \n",
    "    # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "    tweets_df.to_csv('{}-tweets.csv'.format(username), sep=',', index = False)\n",
    "#     tweets_df.to_excel('{}-tweets.xlsx'.format(username), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating example username to scrape from\n",
    "username = 'billgates'\n",
    "\n",
    "# Max recent tweets pulls x amount of most recent tweets from that user\n",
    "max_tweets = 150\n",
    "\n",
    "# Function will scrape username, attempt to pull max_tweet amount, and create csv/excel file from data.\n",
    "scrape_extended_tweets(username,max_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query by Text Search\n",
    "I created one function to build off of for scraping tweets by text search.\n",
    "\n",
    "#### F1. scrape_text_query\n",
    "This function scrapes tweets from Twitter based on the text search and exports the data as a csv or excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_text_query(text_query, max_tweets):\n",
    "    # Creation of query method using parameters\n",
    "    tweets = tweepy.Cursor(api.search,q=text_query, tweet_mode='extended').items(max_tweets)\n",
    "\n",
    "    # List comprehension pulling chosen tweet information from tweets iterable object\n",
    "    # Add or remove tweet information you want in the below list comprehension\n",
    "    tweets_list = [[tweet.full_text, tweet.created_at, tweet.id_str, tweet.user.screen_name, tweet.coordinates,\n",
    "               tweet.place, tweet.retweet_count, tweet.favorite_count, tweet.lang,\n",
    "               tweet.source, tweet.in_reply_to_status_id_str, \n",
    "                tweet.in_reply_to_user_id_str, tweet.is_quote_status,\n",
    "                ] for tweet in tweets]\n",
    "\n",
    "    # Creation of dataframe from tweets_list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list,columns=['Tweet Text', 'Tweet Datetime', 'Tweet Id', 'Twitter @ Name', 'Tweet Coordinates', 'Place Info',\n",
    "                                                 'Retweets', 'Favorites', 'Language', 'Source', 'Replied Tweet Id',\n",
    "                                                  'Replied Tweet User Id Str', 'Quote Status Bool'])\n",
    "\n",
    "    # Checks if there are coordinates attached to tweets, if so extracts them\n",
    "    tweets_df['Tweet Coordinates'] = tweets_df.apply(extract_coordinates,axis=1)\n",
    "    \n",
    "    # Checks if there is place information available, if so extracts them\n",
    "    tweets_df['Place Info'] = tweets_df.apply(extract_place,axis=1)\n",
    "\n",
    "    # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "    tweets_df.to_csv('{}-tweets.csv'.format(text_query), sep=',', index = False)\n",
    "#     tweets_df.to_excel('{}-tweets.xlsx'.format(text_query), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input search query to scrape tweets and name csv file\n",
    "text_query = 'Coronavirus'\n",
    "\n",
    "# Max recent tweets pulls x amount of most recent tweets from that user\n",
    "max_tweets = 150\n",
    "\n",
    "# Function scrapes for tweets containing text_query, attempting to pull max_tweet amount and create csv/excel file containing data.\n",
    "scrape_text_query(text_query, max_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting User Information From Tweets<a name=\"Section2\"></a>\n",
    "[Return to Table of Contents](#TOC)\n",
    "\n",
    "<b>Tweepy excels in this category. Having more access to user information than GetOldTweets3.</b>\n",
    "<br>List of information available in user object with Tweepy. This is not an exhaustive list but does contain a majority of the available information. If you want an exhaustive list of everything contained in the tweet object there's documentation [here](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/user-object) describing all the attributes. \n",
    "\n",
    "String versions of Id's (e.g., id_str, user.id_str) are used instead to best keep data integrity as there is a possibility for Id's stored as integers to be cut off.\n",
    "\n",
    "* tweet.text: Text content of tweet\n",
    "* tweet.created_at: Date tweet was created\n",
    "* tweet.id_str: Id of tweet\n",
    "* tweet.user.name: Name of the user as they've defined it\n",
    "* tweet.user.screen_name: Username of tweet's author, commonly called User @ name\n",
    "* tweet.user.id_str: Use id of tweet's author\n",
    "* tweet.user.location: User defined location for account's profile. Can be nullable\n",
    "* tweet.user.url: URL provided by user in bio. Can be nullable\n",
    "* tweet.user.description: Text in user bio. Can be nullable\n",
    "* tweet.user.verified: Boolean indicating whether user has a verified account\n",
    "* tweet.user.followers_count: Count of followers user has\n",
    "* tweet.user.friends_count: Count of other users that user is following\n",
    "* tweet.user.favourites_count: Count of tweets user has liked in the account's lifetime\n",
    "* tweet.user.statuses_count: Count of tweets (including retweets) issued by user\n",
    "* tweet.user.listed_count: Count of public lists that user is member of\n",
    "* tweet.user.created_at: Date that the user account was created on Twitter\n",
    "* tweet.user.profile_image_url_https: HTTPS-based URL pointing to user's profile image\n",
    "* tweet.user.default_profile: When true, indicates user has not altered the theme or background of user profile\n",
    "* tweet.user.default_profile_image: When true, indicates if user has not uploaded their own profile image and default image is used instead\n",
    "\n",
    "### Query by Text Search\n",
    "I created one function to build off of that searches by text and pulls all user information available.\n",
    "\n",
    "#### F1. scrape_user_information\n",
    "This function scrapes tweets from Twitter based on the text search, pulls user information and exports the data as a csv or excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_user_information(text_query, max_tweets):\n",
    "    # Creation of query method using parameters\n",
    "    tweets = tweepy.Cursor(api.search,q=text_query).items(max_tweets)\n",
    "\n",
    "    # List comprehension pulling chosen tweet information from tweets iterable object\n",
    "    # Add or remove tweet information you want in the below list comprehension\n",
    "    tweets_list = [[tweet.text, tweet.created_at, tweet.id_str, tweet.user.name, tweet.user.screen_name, \n",
    "                    tweet.user.id_str, tweet.user.location, tweet.user.url,\n",
    "                    tweet.user.description, tweet.user.verified, tweet.user.followers_count,\n",
    "                    tweet.user.friends_count, tweet.user.favourites_count, tweet.user.statuses_count,\n",
    "                    tweet.user.listed_count, tweet.user.created_at, tweet.user.profile_image_url_https,\n",
    "                    tweet.user.default_profile, tweet.user.default_profile_image] for tweet in tweets]\n",
    "\n",
    "    # Creation of dataframe from tweets_list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list,columns=['Tweet Text', 'Tweet Datetime', 'Tweet Id', 'Twitter Username', 'Twitter @ name',\n",
    "                                                 'Twitter User Id', 'Twitter User Location', 'URL in Bio', 'Twitter Bio',\n",
    "                                                 'User Verified Status', 'Users Following Count',\n",
    "                                                 'Number users this account is following', 'Users Number of Likes', 'Users Tweet Count',\n",
    "                                                 'Lists Containing User', 'Account Created Time', 'Profile Image URL',\n",
    "                                                 'User Default Profile', 'User Default Profile Image'])\n",
    "\n",
    "    # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "    tweets_df.to_csv('{}-userinfo-tweets.csv'.format(text_query), sep=',', index = False)\n",
    "    # tweets_df.to_excel('{}-userinfo-tweets.xlsx'.format(text_query), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Input search query to scrape tweets and name csv file\n",
    "text_query = 'Coronavirus'\n",
    "\n",
    "# Max recent tweets pulls x amount of most recent tweets from that user\n",
    "max_tweets = 150\n",
    "\n",
    "# Function scrapes for tweets containing text_query, attempting to pull max_tweet amount and create csv/excel file containing data.\n",
    "scrape_user_information(text_query, max_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scraping Tweets With Advanced Queries<a name=\"Section3\"></a>\n",
    "[Return to Table of Contents](#TOC)\n",
    "<br>List of query methods available with Tweepy. This is not an exhaustive list but does contain a majority of the methods available. If you want an exhaustive list of everything available there's documentation [here](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets).\n",
    "\n",
    "* q = str: Setting query based on text\n",
    "* geocode = str \"lat,long,radius\": Setting location of query and radius\n",
    "* lang = str: Setting language of query, full list of language codes [here](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)</b>\n",
    "* result_type = str \"mixed\"/\"recent\"/\"popular\": Setting popularity preference of query\n",
    "* until = str \"yyyy-mm-dd\": Setting upper bound date on query, if using standard search API be cognizant of 7-day limit\n",
    "* since_id = str or int: Returns results with Id's more recent than given Id\n",
    "* max_id = str or int: Returns results with Id's older than given Id\n",
    "* count = int: Number of tweets to return per page. Max is 100, defaults to 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a function to build off of based that utilize the different query methods available with Tweepy. As you can see you can mix and match the above methods in any way. It's important to remember that the more restrictive you make the search the more likely that a smaller amount of tweets that will come up.\n",
    "\n",
    "#### F1. scrape_advanced_queries\n",
    "This function queries by using geocode to set a location to query for tweets and restrict within a certain radius, lang to scrape for tweets written in a specific language, result_type to search for tweets based on popularity, until to set an upper bound date on tweets, since_id to set a restriction on the oldest id possible, and max_id to set a restriction on the earliest id possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_advanced_queries(coordinates, language, result_type, until_date, max_tweets):\n",
    "    # Creation of query method using parameters\n",
    "    tweets = tweepy.Cursor(api.search, geocode=coordinates, lang=language, result_type = result_type, \n",
    "                           until = until_date, count = 100).items(max_tweets)\n",
    "\n",
    "    # List comprehension pulling chosen tweet information from tweets iterable object\n",
    "    # Add or remove tweet information you want in the below list comprehension\n",
    "    tweets_list = [[tweet.text, tweet.created_at, tweet.id_str, tweet.favorite_count, tweet.user.screen_name, \n",
    "                    tweet.user.id_str, tweet.user.location, tweet.user.url, \n",
    "                    tweet.user.verified, tweet.user.followers_count,\n",
    "                    tweet.user.friends_count, tweet.user.statuses_count,\n",
    "                    tweet.user.default_profile_image, tweet.lang] for tweet in tweets]\n",
    "\n",
    "    # Creation of dataframe from tweets_list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list,columns=['Tweet Text', 'Tweet Datetime', 'Tweet Id', 'Tweet Favorite Count', 'Twitter @ name',\n",
    "                                                 'Twitter User Id', 'Twitter User Location', 'URL in Bio','User Verified Status', 'Users Current Following Count',\n",
    "                                                 'Number of accounts user is following', 'Users Tweet Count',\n",
    "                                                 'Profile Image URL','Tweet Language'])\n",
    "    \n",
    "    # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "    tweets_df.to_csv('advancedqueries-tweets.csv', sep=',', index = False)\n",
    "    # tweets_df.to_excel('advancedqueries-tweets.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example may no longer show tweets if until_date falls outside \n",
    "# of 7-day period from when you run cell\n",
    "\n",
    "coordinates = '19.402833,-99.141051,50mi'\n",
    "language = 'es'\n",
    "result_type = 'recent'\n",
    "until_date = '2020-08-10'\n",
    "max_tweets = 150\n",
    "\n",
    "scrape_advanced_queries(coordinates, language, result_type, until_date, max_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Putting It All Together<a name=\"Section4\"></a>\n",
    "[Return to Table of Contents](#TOC)\n",
    "<br>\n",
    "Great, we now know how to pull more information from tweets and querying with advanced parameters. The great thing is how easy it is to mix and match whatever you want to search for. While it was shown above several times. The point is that you can mix and match the information you want from the tweets and the type of queries you conduct. It's just important that you update the column names in the pandas dataframe so you don't get errors.\n",
    "\n",
    "<br>\n",
    "Below is an example of a search for 150 tweets with 'Coronavirus' in it that occurred within a 50 mile radius of Las Vegas, NV. Which in this case has the geo coordinates of lat 36.169786, long -115.139858\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = 'Coronavirus'\n",
    "coordinates = '36.169786,-115.139858,50mi'\n",
    "max_tweets = 150\n",
    "\n",
    "# Creation of query method using parameters\n",
    "tweets = tweepy.Cursor(api.search, q = text_query, geocode = coordinates, count = 100).items(max_tweets)\n",
    "\n",
    "# List comprehension pulling chosen tweet information from tweets iterable object\n",
    "# Add or remove tweet information you want in the below list comprehension\n",
    "tweets_list = [[tweet.text, tweet.created_at, tweet.id_str, tweet.favorite_count, tweet.user.screen_name, \n",
    "                tweet.user.id_str, tweet.user.location, tweet.user.followers_count, tweet.coordinates, tweet.place] for tweet in tweets]\n",
    "\n",
    "# Creation of dataframe from tweets_list\n",
    "# Add or remove columns as you remove tweet information\n",
    "tweets_df = pd.DataFrame(tweets_list,columns=['Tweet Text', 'Tweet Datetime', 'Tweet Id', 'Tweet Favorite Count', 'Twitter @ name',\n",
    "                                             'Twitter User Id', 'Twitter User Location', 'Users Current Following Count', 'Tweet Coordinates', 'Place Info'])\n",
    "\n",
    "# Checks if there are coordinates attached to tweets, if so extracts them\n",
    "tweets_df['Tweet Coordinates'] = tweets_df.apply(extract_coordinates,axis=1)\n",
    "    \n",
    "# Checks if there is place information available, if so extracts them\n",
    "tweets_df['Place Info'] = tweets_df.apply(extract_place,axis=1)\n",
    "\n",
    "# Uncomment/comment below lines to decide between creating csv or excel file \n",
    "tweets_df.to_csv('put-together-tweets.csv', sep=',', index = False)\n",
    "# tweets_df.to_excel('put-together-tweets.xlsx', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
