{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Companion Notebook for Scraping Twitter Using GetOldTweets3\n",
    "\n",
    "Package: https://github.com/Mottl/GetOldTweets3\n",
    "\n",
    "Article Read-Along: https://towardsdatascience.com/how-to-scrape-more-information-from-tweets-on-twitter-44fd540b8a1f\n",
    "\n",
    "### Notebook Author: Martin Beck\n",
    "#### Information current as of August, 13th 2020\n",
    "<b> Dependencies:</b> Make sure GetOldTweets3 is already installed in your Python environment. If not, you can pip install GetOldTweets3 to install the package. If you want more information on setting up I have an article [here](https://towardsdatascience.com/how-to-scrape-tweets-from-twitter-59287e20f0f1) that goes into deeper detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook's Table of Contents<a name=\"TOC\"></a>\n",
    "<br>\n",
    "<b>This companion notebook is meant to build on the scraping article and article notebook as it covers more scenarios that may come up and provides more examples.</b>\n",
    "\n",
    "1. [Getting More Information From Tweets](#Section1)\n",
    "<br>How to scrape more information from tweets such as favorite count, retweet count, mentions, permalinks, etc.\n",
    "2. [Getting User Information From Tweets](#Section2)\n",
    "<br><b>GetOldTweets3 does not offer</b> anymore user information than their screename or Twitter @ name which is shown in section 1.\n",
    "3. [Scraping Tweets With Advanced Queries](#Section3)\n",
    "<br>How to scrape for tweets using deeper queries such as searching by language of tweets, tweets within a certain location, tweets within specific date ranges, top tweets, etc.\n",
    "4. [Putting It All Together](#Section4)\n",
    "<br>Showcasing how you can mix and match the methods shown above to create queries that'll fulfill your data needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports for Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pip install GetOldTweets3 if you don't already have the package\n",
    "# !pip install GetOldTweets3\n",
    "\n",
    "# Imports\n",
    "import GetOldTweets3 as got\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting More Information From Tweets <a name=\"Section1\"></a>\n",
    "[Return to Table of Contents](#TOC)\n",
    "<br>\n",
    "List of information available in the tweet object with GetOldTweets3 I included everything except geo data due to issues that are currently still open.\n",
    "\n",
    "* tweet.geo: <b>*NOTE GEO-DATA NOT WORKING BASED ON ISSUE</b><br><br>\n",
    "\n",
    "* tweet.id: Id of tweet\n",
    "* tweet.author_id: User id of tweet's author\n",
    "* tweet.username: Username of tweet's author, commonly called User's @ name\n",
    "* tweet.to: If tweet is a reply, the original tweet's username\n",
    "* tweet.text: Text content of tweet\n",
    "* tweet.retweets: Count of retweets\n",
    "* tweet.favorites: Count of favorites\n",
    "* tweet.replies: Count of replies\n",
    "* tweet.date: Date tweet was created\n",
    "* tweet.formatted_date: Formatted version of when tweet was created\n",
    "* tweet.hashtags: Hashtags that tweet contains\n",
    "* tweet.mentions: Mentions of other users that tweet contains\n",
    "* tweet.urls: Urls that are in the tweet\n",
    "* tweet.permalink: Permalink of tweet itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query by Username\n",
    "I created three functions to build off of based off of various scenarios that are likely to happen for someone scraping tweets from users. After each function I call them to showcase an example of them being used.\n",
    "\n",
    "#### F1. scrape_user_tweets\n",
    "This function scrapes a single users tweets and exports the data as a csv or excel file\n",
    "\n",
    "#### F2. scrape_multiple_users_multifile\n",
    "This function scrapes multiple users based on a list and exports separate csv or excel files per user.\n",
    "\n",
    "#### F3. scrape_multiple_users_singlefile\n",
    "This function scrapes multiple users based on a list and exports one csv or excel file containing all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_user_tweets(username, max_tweets):\n",
    "    # Creation of query object\n",
    "    tweetCriteria = got.manager.TweetCriteria().setUsername(username)\\\n",
    "                                            .setMaxTweets(max_tweets)\n",
    "    # Creation of list that contains all tweets\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "    # Pulling information from tweets iterable object\n",
    "    # Add or remove tweet information you want in the below list comprehension\n",
    "    tweets_list = [[tweet.id, tweet.author_id, tweet.username, tweet.to, tweet.text, tweet.retweets, tweet.favorites,\n",
    "                    tweet.replies,tweet.date, tweet.formatted_date, tweet.hashtags, \n",
    "                    tweet.mentions, tweet.urls, tweet.permalink,] for tweet in tweets]\n",
    "\n",
    "    # Creation of dataframe from tweets list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns = ['Tweet Id', 'Tweet User Id', 'Tweet User','Reply to', 'Text','Retweets', 'Favorites', 'Replies', 'Datetime',\n",
    "                                                     'Formatted date', 'Hashtags','Mentions','Urls','Permalink'])\n",
    "    \n",
    "    # Removing timezone information to allow excel file download\n",
    "    tweets_df['Datetime'] = tweets_df['Datetime'].apply(lambda x: x.replace(tzinfo=None))\n",
    "    \n",
    "    # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "    tweets_df.to_csv('{}-tweets.csv'.format(username), sep=',', index = False)\n",
    "#     tweets_df.to_excel('{}-tweets.xlsx'.format(username), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating example username to scrape from\n",
    "username = 'jack'\n",
    "\n",
    "# Max recent tweets pulls x amount of most recent tweets from that user\n",
    "max_tweets = 150\n",
    "\n",
    "# Function will scrape username, attempt to pull max_tweet amount, and create csv/excel file from data.\n",
    "scrape_user_tweets(username,max_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_users_multifile(username_list, max_tweets_per):\n",
    "    # Looping through each username in user list\n",
    "    for username in username_list:\n",
    "        # Creation of query object\n",
    "        tweetCriteria = got.manager.TweetCriteria().setUsername(username)\\\n",
    "                                                .setMaxTweets(max_tweets_per)\n",
    "        # Creation of list that contains all tweets\n",
    "        tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "        # Creating list of chosen tweet data\n",
    "        # Add or remove tweet information you want in the below list comprehension\n",
    "        tweets_list = [[tweet.id, tweet.author_id, tweet.username, tweet.to, tweet.text, tweet.retweets, tweet.favorites,\n",
    "                        tweet.replies,tweet.date, tweet.formatted_date, tweet.hashtags, \n",
    "                        tweet.mentions, tweet.urls, tweet.permalink,] for tweet in tweets]\n",
    "\n",
    "        # Creation of dataframe from tweets list\n",
    "        # Add or remove columns as you remove tweet information\n",
    "        tweets_df = pd.DataFrame(tweets_list, columns = ['Tweet Id', 'Tweet User Id', 'Tweet User','Reply to', 'Text','Retweets', 'Favorites', 'Replies', 'Datetime',\n",
    "                                                         'Formatted date', 'Hashtags','Mentions','Urls','Permalink'])\n",
    "        \n",
    "        # Removing timezone information to allow excel file download\n",
    "        tweets_df['Datetime'] = tweets_df['Datetime'].apply(lambda x: x.replace(tzinfo=None))\n",
    "        \n",
    "        # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "        tweets_df.to_csv('{}-tweets.csv'.format(username), sep=',', index = False)\n",
    "#         tweets_df.to_excel('{}-tweets.xlsx'.format(username), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating example user list with 3 users\n",
    "user_name_list = ['jack','billgates','random']\n",
    "\n",
    "# Max recent tweets pulls x amount of most recent tweets from that user\n",
    "max_tweets_per = 150\n",
    "\n",
    "# Function will scrape each user, attempting to pull max_tweet amount, and create csv/excel file per user.\n",
    "scrape_multiple_users_multifile(user_name_list, max_tweets_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_users_singlefile(username_list, max_tweets_per):\n",
    "    # Creating master list to contain all tweets\n",
    "    master_tweets_list = []\n",
    "    \n",
    "    # Looping through each username in user list\n",
    "    for username in user_name_list:\n",
    "        # Creation of query object\n",
    "        tweetCriteria = got.manager.TweetCriteria().setUsername(username)\\\n",
    "                                                .setMaxTweets(max_tweets_per)\n",
    "        # Creation of list that contains all tweets\n",
    "        tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "        # Creating list of chosen tweet data\n",
    "        # Appending new tweets per user into the master tweet list\n",
    "        # Add or remove tweet information you want in the below list comprehension\n",
    "        for tweet in tweets:\n",
    "            master_tweets_list.append((tweet.id, tweet.author_id, tweet.username, tweet.to, tweet.text, tweet.retweets, tweet.favorites,\n",
    "                            tweet.replies,tweet.date, tweet.formatted_date, tweet.hashtags, \n",
    "                            tweet.mentions, tweet.urls, tweet.permalink))\n",
    "\n",
    "    # Creation of dataframe from tweets list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(master_tweets_list, columns = ['Tweet Id', 'Tweet User Id', 'Tweet User','Reply to', 'Text','Retweets', 'Favorites', 'Replies', 'Datetime',\n",
    "                                                             'Formatted date', 'Hashtags','Mentions','Urls','Permalink'])\n",
    "    \n",
    "    # Removing timezone information to allow excel file download\n",
    "    tweets_df['Datetime'] = tweets_df['Datetime'].apply(lambda x: x.replace(tzinfo=None))\n",
    "    \n",
    "    # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "    tweets_df.to_csv('multi-user-tweets.csv', sep=',', index = False)\n",
    "#     tweets_df.to_excel('multi-user-tweets.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating example user list with 3 users\n",
    "user_name_list = ['jack','billgates','random']\n",
    "\n",
    "# Max recent tweets pulls x amount of most recent tweets from that user\n",
    "max_tweets_per = 150\n",
    "\n",
    "# Function will scrape each user, attempting to pull max_tweet amount, and create one csv/excel file containing all data name multi-user-tweets.\n",
    "scrape_multiple_users_singlefile(user_name_list, max_tweets_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query by Text Search\n",
    "I created a function to build off of for scraping tweets by text search.\n",
    "\n",
    "#### F1. scrape_text_query\n",
    "This function scrapes tweets from Twitter based on the text search and exports the data as a csv or excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_text_query(text_query, count):\n",
    "    # Creation of query object\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query)\\\n",
    "                            .setMaxTweets(count)\n",
    "    # Creation of list that contains all tweets\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "    # Creating list of chosen tweet data\n",
    "    # Add or remove tweet information you want in the below list comprehension\n",
    "    tweets_list = [[tweet.id, tweet.author_id, tweet.username, tweet.to, tweet.text, tweet.retweets, tweet.favorites,\n",
    "                    tweet.replies,tweet.date, tweet.formatted_date, tweet.hashtags, \n",
    "                    tweet.mentions, tweet.urls, tweet.permalink,] for tweet in tweets]\n",
    "\n",
    "    # Creation of dataframe from tweets\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns = ['Tweet Id', 'Tweet User Id', 'Tweet User','Reply to', 'Text','Retweets', 'Favorites', 'Replies', 'Datetime',\n",
    "                                                             'Formatted date', 'Hashtags','Mentions','Urls','Permalink'])\n",
    "    \n",
    "    # Removing timezone information to allow excel file download\n",
    "    tweets_df['Datetime'] = tweets_df['Datetime'].apply(lambda x: x.replace(tzinfo=None))\n",
    "    \n",
    "    # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "    tweets_df.to_csv('{}-tweets.csv'.format(text_query), sep=',', index = False)\n",
    "#     tweets_df.to_excel('{}-tweets.xlsx'.format(text_query), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input search query to scrape tweets and name csv file\n",
    "text_query = 'Coronavirus'\n",
    "\n",
    "# Max recent tweets pulls x amount of most recent tweets from that user\n",
    "max_tweets = 150\n",
    "\n",
    "# Function scrapes for tweets containing text_query, attempting to pull max_tweet amount and create csv/excel file containing data.\n",
    "scrape_text_query(text_query, max_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting User Information From Tweets<a name=\"Section2\"></a>\n",
    "[Return to Table of Contents](#TOC)\n",
    "<br><b>GetOldTweets3 is limited in the user information that is accessible.</b> This library only allows access to a tweet author's username and user_id. If you want user information I recommend looking into utilizing Tweepy for all of your scraping, or using Tweepy in tandem with GetOldTweets3 in order to utilize both libraries to their strengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scraping Tweets With Advanced Queries<a name=\"Section3\"></a>\n",
    "[Return to Table of Contents](#TOC)\n",
    "<br>\n",
    "List of methods available with GetOldTweets3 to refine your queries.\n",
    "\n",
    "* setUsername(str): Setting query based on username\n",
    "* setMaxTweets(int): Setting maximum number of tweets to search\n",
    "* setQuerySearch(str): Setting query based on text\n",
    "* setSince(str \"yyyy-mm-dd\"): Setting lower bound date on query\n",
    "* setUntil(str \"yyyy-mm-dd\"): Setting upper bound date on query\n",
    "* setNear(str): Setting location of query search\n",
    "* setWithin(str): Setting radius of query search location\n",
    "* setLang(str): Setting language of query\n",
    "* setTopTweets(bool): Setting query to search only for top tweets\n",
    "* setEmoji(\"ignore\"/\"unicode\"/\"name\"): Setting query to search using emoji styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created two functions to build off of that utilize the different query methods available through the TweetCriteria class. As you can see you can mix and match the above methods in any way. It's important to remember that the more restrictive you make the search the more likely that a smaller amount of tweets that will come up.\n",
    "\n",
    "#### F1. scrape_advanced_queries1\n",
    "This function queries by using .setUsername to set the username, .setQuerySearch to set text to query for, .setSince to set the oldest date of the tweets to query, .setUntil to set the most recent date of the tweets to query, .setMaxTweets to set the amount of tweets to query for.\n",
    "\n",
    "#### F2. scrape_advanced_queries2\n",
    "This function queries by using .setQuerySearch, .setNear to set a location to query for tweets around, .setWithin to set a radius restriction around the chosen location, .setLang to scrape for tweets written in a specific language, .setMaxTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_advanced_queries1(username, text_query, since_date, until_date, count):\n",
    "    # Creation of query object with as many specific queries as you want\n",
    "    tweetCriteria = got.manager.TweetCriteria().setUsername(username)\\\n",
    "    .setQuerySearch(text_query).setSince(since_date)\\\n",
    "    .setUntil(until_date).setMaxTweets(count)\n",
    "    \n",
    "    # Creation of list that contains all tweets\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "    # Creating list of chosen tweet data\n",
    "    # Add or remove tweet information you want in the below list comprehension\n",
    "    tweets_list = [[tweet.id, tweet.author_id, tweet.username, tweet.to, tweet.text, tweet.retweets, tweet.favorites,\n",
    "                    tweet.replies,tweet.date, tweet.formatted_date, tweet.hashtags, \n",
    "                    tweet.mentions, tweet.urls, tweet.permalink,] for tweet in tweets]\n",
    "\n",
    "    # Creation of dataframe from tweets list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns = ['Tweet Id', 'Tweet User Id', 'Tweet User','Reply to', 'Text','Retweets', 'Favorites', 'Replies', 'Datetime',\n",
    "                                                     'Formatted date', 'Hashtags','Mentions','Urls','Permalink'])\n",
    "    \n",
    "    # Removing timezone information to allow excel file download\n",
    "    tweets_df['Datetime'] = tweets_df['Datetime'].apply(lambda x: x.replace(tzinfo=None))\n",
    "    \n",
    "    # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "    tweets_df.to_csv('{}-tweets.csv'.format(username), sep=',', index = False)\n",
    "#     tweets_df.to_excel('{}-tweets.xlsx'.format(username), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"BarackObama\"\n",
    "text_query = \"Hello\"\n",
    "since_date = \"2011-01-01\"\n",
    "until_date = \"2016-12-20\"\n",
    "count = 150\n",
    "\n",
    "scrape_advanced_queries1(username, text_query, since_date, until_date, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_advanced_queries2(text_query, location, radius, language, count):\n",
    "    # Creation of query object with as many specific queries as you want\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query)\\\n",
    "    .setNear(location).setWithin(radius).setLang(language).setMaxTweets(count)\n",
    "    \n",
    "    # Creation of list that contains all tweets\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "    # Creating list of chosen tweet data\n",
    "    # Add or remove tweet information you want in the below list comprehension\n",
    "    tweets_list = [[tweet.id, tweet.author_id, tweet.username, tweet.to, tweet.text, tweet.retweets, tweet.favorites,\n",
    "                    tweet.replies,tweet.date, tweet.formatted_date, tweet.hashtags, \n",
    "                    tweet.mentions, tweet.urls, tweet.permalink,] for tweet in tweets]\n",
    "\n",
    "    # Creation of dataframe from tweets list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns = ['Tweet Id', 'Tweet User Id', 'Tweet User','Reply to', 'Text','Retweets', 'Favorites', 'Replies', 'Datetime',\n",
    "                                                     'Formatted date', 'Hashtags','Mentions','Urls','Permalink'])\n",
    "    \n",
    "    # Removing timezone information to allow excel file download\n",
    "    tweets_df['Datetime'] = tweets_df['Datetime'].apply(lambda x: x.replace(tzinfo=None))\n",
    "    \n",
    "    # Uncomment/comment below lines to decide between creating csv or excel file \n",
    "    tweets_df.to_csv('{}-tweets.csv'.format(text_query), sep=',', index = False)\n",
    "    tweets_df.to_excel('{}-tweets.xlsx'.format(text_query), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = \"Hola\"\n",
    "location = \"Mexico\"\n",
    "radius = \"100mi\"\n",
    "language = \"Spanish\"\n",
    "count = 150\n",
    "\n",
    "scrape_advanced_queries2(text_query, location, radius, language, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Putting It All Together<a name=\"Section4\"></a>\n",
    "[Return to Table of Contents](#TOC)\n",
    "<br>\n",
    "Great, we now know how to pull more information from tweets and querying with advanced parameters. The great thing is how easy it is to mix and match whatever you want to search for. While it was shown above several times. The point is that you can mix and match the information you want from the tweets and the type of queries you conduct. It's just important that you update the column names in the pandas dataframe so you don't get errors.\n",
    "\n",
    "<br>\n",
    "Below is an example of a search for 150 top tweets with 'coronavirus' in it that occurred between August 5th and August 8th 2020 in Washington D.C.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = 'Coronavirus'\n",
    "since_date = '2020-08-05'\n",
    "until_date = '2020-08-10'\n",
    "location = 'Washington, D.C.'\n",
    "top_tweets = True\n",
    "count = 150\n",
    "\n",
    "# Creation of tweetCriteria query object with methods to specify further\n",
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setSince(since_date)\\\n",
    ".setUntil(until_date).setNear(location).setTopTweets(top_tweets).setMaxTweets(count)\n",
    "\n",
    "# Creation of tweets iterable containing all queried tweet data\n",
    "tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "# List comprehension pulling chosen tweet information per tweet from tweets\n",
    "# Add or remove tweet information you want in the below list comprehension\n",
    "tweets_list = [[tweet.id, tweet.author_id, tweet.username, tweet.to, tweet.text, tweet.retweets, tweet.favorites,\n",
    "                tweet.replies,tweet.date, tweet.mentions, tweet.urls, tweet.permalink,] \n",
    "               for tweet in tweets]\n",
    "\n",
    "# Creation of dataframe from tweets list\n",
    "# Add or remove columns as you remove tweet information\n",
    "tweets_df = pd.DataFrame(tweets_list, columns = ['Tweet Id', 'Twitter User Id', 'Twitter @ Name','Reply to', 'Text','Retweets', 'Favorites', \n",
    "                                                 'Replies', 'Datetime','Mentions','Urls','Permalink'])\n",
    "# Removing timezone information to allow excel file download\n",
    "tweets_df['Datetime'] = tweets_df['Datetime'].apply(lambda x: x.replace(tzinfo=None))\n",
    "\n",
    "# Uncomment/comment below lines to decide between creating csv or excel file \n",
    "tweets_df.to_csv('put-together-tweets.csv', sep=',', index = False)\n",
    "# tweets_df.to_excel('put-together-tweets.xlsx', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
